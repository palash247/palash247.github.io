<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Palash Karmore</title>
    <description>A blog for my projects.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 30 Jul 2018 18:04:44 +0000</pubDate>
    <lastBuildDate>Mon, 30 Jul 2018 18:04:44 +0000</lastBuildDate>
    <generator>Jekyll v3.6.2</generator>
    
      <item>
        <title>100 days of ML code logs</title>
        <description>&lt;p&gt;My everyday progress logs of the #100daysofMLCode challenge.&lt;/p&gt;

&lt;h3 id=&quot;date-27072018&quot;&gt;Date: 27/07/2018:&lt;/h3&gt;

&lt;p&gt;For the day, I revisited the derivation for the backpropagation algorithm. I find myself struggling with writing differential equations for matrices. The only way I could possibly come up with the proper differentiation is by matching the dimensions of the matrices. I don’t know if it’s the correct way of doing Linear Algebra-Calculus, but for now, it is working for me. For reference, I’ve used the first course of Andrew Ng’s Deep Learning specialization from Coursera.&lt;/p&gt;

&lt;h3 id=&quot;date-28072018&quot;&gt;Date: 28/07/2018&lt;/h3&gt;

&lt;p&gt;Read an article by Randal J. Barnes about Matrix Differentiation. I found it interesting and now I know how to backpropagate without matching the dimensions of the matrices involved in the differentiation. Then I went back to the maths of forwarding and backward propagation of Deep-L layer neural network from Andrew Ng’s Deep Learning course. Now I feel a little more confident about the derivation and I hope I would not forget it anytime soon. I think I should make some flashcards about this real soon.&lt;/p&gt;

&lt;h3 id=&quot;date-29072018&quot;&gt;Date: 29/07/2018&lt;/h3&gt;

&lt;p&gt;Found a way to download assignments from Coursera, as they don’t let you redo your assignments after completing the certifications. Now, I have a copy of every ipython notebook assignments of Deep Learning course. I don’t know if its illegal, but I find it helpful to solve problems while revising, so I am going to keep them. For the day, I’ve completed the Deep-L laver NN assignment from the first course of Deep Learning Specialization. And there’s one more project I am working on which allows you to schedule a message on a message platform. Today, I’ve explored different third-party APIs to automate message sending on WhatsApp and then I decided to go with Selenium web driver as it appears to all the third party APIs are doing it the same way. I’ve successfully shared some text message through Selenium but then I had to grant the access permission every time I ran the application which kind of sucks. I found some solutions for it and I’ve tried some without any luck I will try some more soon. Also, I learned how to set up a VNC server on a cheap digital ocean droplet which is going to help me with this selenium automation. Another thing is I am trying to wake up at 4 am but I am struggling with going to sleep early. I will keep on trying though. That’s it for the day.&lt;/p&gt;

&lt;h3 id=&quot;date-30072018&quot;&gt;Date: 30/07/2018&lt;/h3&gt;

&lt;p&gt;Today, I struggled to wake up at 4 am again. Next day I am going to try harder. For the day, I’ve spent time configuring Twilio with the chatbot I am building for my company. Before that, I was engaged in the generating more stories (conversation flows) for the bot to tackle some mistakes it was doing. Before that when I woke up at 6 am I went through some modular way to implement fully connected NN with any architecture. Tomorrow I am going to try a little harder to wake up at 4.&lt;/p&gt;
</description>
        <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
        <link>http://localhost:4000/articles/2018-07/100-days-of-ML-code-logs</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2018-07/100-days-of-ML-code-logs</guid>
        
        
        <category>logs</category>
        
      </item>
    
      <item>
        <title>Artificial Neural Networks from scratch (sort off)</title>
        <description>&lt;h2 id=&quot;what-are-artificial-neural-networks&quot;&gt;What are Artificial Neural Networks?&lt;/h2&gt;

&lt;p&gt;It comprises of Linear Algebra operations, non-linear activations and some optimization of cost function using high school Calculus to give a computer the capability to learn something from historical data. Let’s try to learn each thing intuitively.&lt;/p&gt;

&lt;h3 id=&quot;optimization&quot;&gt;Optimization:&lt;/h3&gt;

&lt;p&gt;Given a function \(f(x)\), how would you find a minimum of it with respect \(x\). Turns out &lt;em&gt;Gradient/Derivative&lt;/em&gt; of a function can come handy. So what is Derivative of function?&lt;/p&gt;

&lt;h4 id=&quot;grdient&quot;&gt;Grdient:&lt;/h4&gt;

&lt;p&gt;A derivative of function \(f(x)\) with respect to \(x\) represents the rate of change of \(f(x)\) with respect to \(x\). For example, if you are given a function \(d(t)\) representing displacement of a car at time \(t\), the derivative of \(d\) will represent the velocity of the car at time \(t\). A graphical intuition of the derivative is a slope of the tangent at f
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://github.com/jekyll/jekyll-help&quot;&gt;Jekyll’s dedicated Help repository&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Mon, 18 Jun 2018 15:07:19 +0000</pubDate>
        <link>http://localhost:4000/articles/2018-06/artificial-neural-networks-from-scratch-(sort-for)</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2018-06/artificial-neural-networks-from-scratch-(sort-for)</guid>
        
        
        <category>tutorial</category>
        
      </item>
    
  </channel>
</rss>
