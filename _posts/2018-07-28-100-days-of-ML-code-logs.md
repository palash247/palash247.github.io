---
layout: post
title:  100 days of ML code logs
date:   2018-07-28 15:07:19
categories: [logs]
comments: false
---


## Date: 27/07/2018:

For the day, I revisited the derivation for the backpropagation algorithm. I find myself struggling with writing differential equations for matrices. The only way I could possibly come up with the proper differentiation is by matching the dimensions of the matrices. I don't know if it's the correct way of doing Linear Algebra-Calculus, but for now, it is working for me. For reference, I've used the first course of Andrew Ng's Deep Learning specialization from Coursera.

## Date: 28/07/2018

Read an article by Randal J. Barnes about Matrix Differentiation. I found it interesting and now I know how to backpropagate without matching the dimensions of the matrices involved in the differentiation. Then I went back to maths of forward and backward propegation of Deep-L layer neural network from Andrew Ng's Deep Learning course. Now I feel little more confident about the derivation and I hope I would not forget it anytime soon. I think I should make some flash cards about this real soon.
